\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\bx}{{\bf x}}
\newcommand{\bbeta}{\boldsymbol\beta}

\begin{document}

\subsection*{Generic model}

The number of people that are aware of the service at time $t$ is $N(t)$.
This number always increases, $N(t + \Delta t) \geq N(t)$.

At every time, for each person, there is some probability that they
will be riding. This depends on day of week, hour of day, temperature,
etc. The total count for a given time is therefore approximated by:
\[
c(t, \bx) = \sum_i^{N(t)} p_i(t, \bx)
\]
where $p_i(t, \bx) \in [0, 1]$ is the probability that rider $i$ is on
a bike at time $t$ given the conditions $\bx$ (which includes
temperature and all that).

There is an ambiguity between $p_i$ and $N$. Decreasing all $p_i$
while increasing $N$ may leave the count unchanged. The simplest model
will minimize $N$ given the constraint $p_i \in [0, 1]$. This may add
complexity to the cost function.

The problem is, of course, we do not know the distributions
$p_i(t, \bx)$.

Simplify the model by grouping riders into $M$ classes $C_i$, $i = 1
\ldots M$; members of a
class have very similar probability distributions so that we write
\begin{align*}
c(t, \bx) &= \sum_i^M \sum_{j \in C_i} p_{j}(t, \bx) \\
          &= \sum_i^M N_i(t) \bar{p}_i(t, \bx)
\end{align*}
where we have introduced the $M$ class-size parameters $N_i(t) \geq 0$,
which represent
the number of riders in a certain class, and the class rider distribution
is
\[
\bar{p}_i(t, \bx) = \frac{1}{N_i(t)} \sum_{j \in C_i} p_{j}(t, \bx)
\]
where $\bar{p}_i(t, \bx) \in [0, 1]$.

The class sizes satisfy
\[
\sum_i N_i(t) = N(t)
\]
Since riders are free to switch classes, there is no monotonic
constraint on each $N_i(t)$; i.e. they are free to fluctuate.

The goal is to deduce the size and probability distribution for each
class.

In the training dataset, we know the total count
$c(t)$. The error is
\[
e(t) = c(t) - \sum_i^{N(t)} p_i(t, \bx)
\]

\subsection*{Weather effect}

Introduce the weather effect function, $f_W(W, \bbeta_W) \in [0, 1]$, where the
weather score, $W$, given in $\bx$, can take on only 4 values
(integers 1-4).  The weather score introduces a multiplicative effect under
the assumption that bad weather will reduce ridership by a certain
percentage of the fair-weather number.
\[
f_W(W, \bbeta_W) = \delta_{W,1} + \sum_{i=2}^4 \beta_{W,i} \delta_{i,W}
\]
where $W=1$ represents ``ideal''
weather (no rain, storms, etc).

The gradient is
\[
\partial_{\beta_{W,i}} f_W = \delta_{W,i}
\]
for $W > 1$.


We might smooth (average) $W$ 
over several hours, under the assumption that someone caught in a
flash storm on their bike may keep riding or take cover until the
storm passes.

If $W$ is smoothed to $\bar{W}$, a continuous function of $\bar{W}$
must be approximated, for example with polynomials,
\[
f_W(W, \bbeta_W) \approx \sum_{n=1}^N \beta_{W,n} \bar{W}^{n-1}
\]


\subsection*{Day type}

Also introduce a {\it day-type} effect, $f_D(D, \bbeta_D)$, which
considers differences in ridership patterns for working days,
holidays, weekends, etc. We assign an integer value $D \in [0, M]$ for each of these
day types.
\[
g(D, \bbeta_g) = \delta_{D,0} + (1 - \delta_{D,0}) \beta_{g,D}
\]
where $\beta_{g,D} \in [0, \infty)$. Arbitrarily choose $D = 0$ to be the
working day type.

The gradient is
\[
\partial_{\beta_{g,i}} g(D, \bbeta_g) = \delta_{D,i}
\]

Day type will affect recreational and commuter ridership differently,
therefore, we need two day-type effect functions, $g_c$ and $g_r$.

\subsection*{Temperature}

Each input vector $\bx_i$ contains a continuous temperature variable
$T_i$.
Introduce a temperature effect $f_T(T, \bbeta_T)$
that multiplies our count. At an
``ideal'' temperature $\bar{T}$, we have the constraint
\[
f_T(\bar{T}, \bbeta_T) = 1
\]
additionally, we enforce the boundary constraints
\[
f_T(-\infty, \bbeta_T) = f_T(\infty, \bbeta_T) = 0
\]

We implement an asymmetric Gaussian approximation
\[
f_T(T, \bbeta_T) =
\begin{cases}
  e^{-(T - \bar{T})^2 / 2 \Delta T_{\rm low}^2} & T < \bar{T} \\
  e^{-(T - \bar{T})^2 / 2 \Delta T_{\rm hi}^2} & T \geq \bar{T}
\end{cases}
\]
where $\bbeta_T = [\bar{T}, \Delta T_{\rm low}, \Delta T_{\rm hi}]$,
$\Delta T_{\rm low} \geq 0$, and
$\Delta T_{\rm hi} \geq 0$.

The gradient is
\[
\partial_{\bar{T}} f_T =
\begin{cases}
  (T - \bar{T})
    e^{-(T - \bar{T})^2 / 2 \Delta T_{\rm low}^2} / \Delta T_{\rm low}^2
    & T < \bar{T} \\
  (T - \bar{T})
    e^{-(T - \bar{T})^2 / 2 \Delta T_{\rm hi}^2} / \Delta T_{\rm hi}^2
    & T \geq \bar{T}
\end{cases}
\]
and
\[
\partial_{T_{\rm low}} f_T =
\begin{cases}
  (T - \bar{T})^2
    e^{-(T - \bar{T})^2 / 2 \Delta T_{\rm low}^2} /
    \Delta T_{\rm low}^3
    & T < \bar{T} \\
  0 & T \geq \bar{T}
\end{cases}
\]
and
\[
\partial_{T_{\rm hi}} f_T =
\begin{cases}
  0 & T < \bar{T} \\
  (T - \bar{T})^2
    e^{-(T - \bar{T})^2 / 2 \Delta T_{\rm hi}^2} / \Delta T_{\rm hi}^3
    & T \geq \bar{T}
\end{cases}
\]

%\[
%f_T(T, \bbeta_T) =
%\begin{cases}
%  0 & T < T_{\rm low} \\
%  \frac{1}{2} (1 - \cos(\pi (T - T_{\rm low}) /
%  2 \Delta T_{\rm low})) & T_{\rm low} < T < \bar{T} \\
%  \frac{1}{2} (1 - \cos(\pi (T - \bar{T}) /
%  2 \Delta T_{\rm hi})) & \bar{T} < T < T_{\rm hi} \\
%  0 & T > T_{\rm hi}
%\end{cases}
%\]
%where $\bbeta_T = [\bar{T}, \Delta T_{\rm low}, \Delta T_{\rm hi}]$,
%$\Delta T_{\rm low} \geq 0$,
%$\Delta T_{\rm hi} \geq 0$,
%$T_{\rm low} = \bar{T} - \Delta T_{\rm low}$,
%and $T_{\rm hi} = \bar{T} + \Delta T_{\rm hi}$.

%The gradient is
%\[
%\partial_{\bar{T}} f_T =
%\begin{cases}
%  0 & T < T_{\rm low} \\
%  \frac{1}{2} \sin(\pi (T - T_{\rm low}) / 2 \Delta T_{\rm low})) & T_{\rm low} < T < \bar{T} \\
%  \frac{1}{2} (1 - \cos(\pi (T - \bar{T}) /
%  2 \Delta T_{\rm hi})) & \bar{T} < T < T_{\rm hi} \\
%  0 & T > T_{\rm hi}
%\end{cases}
%\]

\subsection*{Popularity}

A {\it popularity} amplitude $p(t, \bbeta)$ determines the amplitude
of the
count predictor function, where $t$ is the date (day).
\[
p(t, \bbeta_p) = \beta_{p,t}
\]
And the gradient is
\[
\partial_{\beta_{p,i}} p(t, \bbeta_p) = \delta_{t,i}
\]





\section*{Training}

We will brute force the minimization of
\[
e = \sqrt{\frac{1}{n} \sum_i^n w_i (\log (1 + c_i) - \log(1 + c(\bx_i,
\bbeta)))^2}
\]

The analytic gradient of the cost function may be useful.
\[
\nabla_{\bbeta} e = - \frac{1}{e n} \sum \frac{w_i e_i}{1 + c(\bx_i,
\bbeta)} \nabla_{\bbeta} c(\bx_i, \bbeta)
\]


\end{document}
